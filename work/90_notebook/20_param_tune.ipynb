{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "import logging\n",
    "from concurrent import futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from ipywidgets import Play, IntSlider, jslink, HBox, interactive_output\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_DIR = os.path.join('/home', 'jovyan', 'work', '01_testset')\n",
    "PRJ_DIR = os.path.join('/home', 'jovyan', 'work', '03_min_block')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マスタの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '01_testset_pre_master.csv'), usecols=['seed'])\n",
    "sys_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '02_testset_sys_master.csv'), usecols=['seed'])\n",
    "stress_seed_df = pd.read_csv(os.path.join(TESTSET_DIR, '03_testset_stress_master.csv'), usecols=['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAMP_TAG = '20230327_1858'\n",
    "CHAMP_DIR = os.path.join('/home', 'jovyan', 'work', 'result', 'champion')\n",
    "\n",
    "champ_path = os.path.join(CHAMP_DIR, 'champ_all_{}.csv'.format(CHAMP_TAG))\n",
    "champ_df = pd.read_csv(champ_path)\n",
    "\n",
    "top_rate = 0.65  # 順位表を参考にチャンピオンスコアを補正(03/28時点)\n",
    "champ_score_dict = {}\n",
    "\n",
    "for _, row in champ_df.iterrows():\n",
    "    seed = row['seed']\n",
    "    score = row['champion_score']\n",
    "\n",
    "    champ_score_dict[seed] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行するロジックの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行プログラムにタグをつけておく\n",
    "PROG_TAG = 'trans_tune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def solve(seed, D, problem, prog_path, invalid_shift, valid_add, valid_shift, valid_redundant, valid_clear):\n",
    "def solve(seed, D, problem, prog_path, invalid_shift, invalid_clear):\n",
    "    problem_path = os.path.join(TESTSET_DIR, 'in', '{:0>4}.txt'.format(seed)) \n",
    "    command_str = 'echo {} {} {}  | {}'.format(problem_path, invalid_shift, invalid_clear, prog_path)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    res = subprocess.run(command_str, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "    # 経過時間(ミリ秒単位)\n",
    "    e_time = time.perf_counter() - start_time\n",
    "    e_time = int(1000 * e_time)    \n",
    "    \n",
    "    #print('{}'.format(prob_id))    \n",
    "    return (seed, D, e_time, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(testset_name, invalid_shift, invalid_clear):\n",
    "    result_df = pd.DataFrame()\n",
    "    future_list = []\n",
    "\n",
    "    logger.info('Start')\n",
    "\n",
    "    testset_path = os.path.join(TESTSET_DIR, testset_name + '_master.csv')\n",
    "    testset_df = pd.read_csv(testset_path)\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        for _, row in testset_df.iterrows():\n",
    "            seed, D, problem = row\n",
    "\n",
    "            # バッチ実行\n",
    "            future = executor.submit(solve, seed=seed,  D=D, problem=problem, prog_path=prog_path, invalid_shift=invalid_shift, invalid_clear=invalid_clear)\n",
    "            future_list.append(future)\n",
    "\n",
    "        _ = futures.as_completed(fs=future_list)\n",
    "\n",
    "    for future in future_list:\n",
    "        seed, D, e_time, res = future.result()\n",
    "\n",
    "        # 結果をまとめる\n",
    "        solve_result = []\n",
    "        \n",
    "        solve_result.append(testset_name)\n",
    "\n",
    "        # 問題パラメタ\n",
    "        solve_result.append(seed)\n",
    "        solve_result.append(D)\n",
    "\n",
    "        # 経過時間\n",
    "        solve_result.append(e_time)\n",
    "        \n",
    "        try:\n",
    "            elem_cnt = 9\n",
    "            \n",
    "            # Result\n",
    "            result = res.stderr.decode('utf-8').split()[-elem_cnt].replace('Result=', '')\n",
    "\n",
    "            # Score\n",
    "            score = int(res.stderr.decode('utf-8').split()[-elem_cnt+1].replace('Score=', ''))\n",
    "            \n",
    "            # 相対スコア\n",
    "            rel_score = int(10 ** 9 * top_rate * champ_score_dict[seed] / score)\n",
    "\n",
    "            # BlockCount\n",
    "            block_cnt = int(res.stderr.decode('utf-8').split()[-elem_cnt+2].replace('BlockCnt=', ''))\n",
    "\n",
    "            # Disuse Block Size\n",
    "            disuse_block_size = int(res.stderr.decode('utf-8').split()[-elem_cnt+3].replace('DisuseBlockSize=', ''))\n",
    "            \n",
    "            # Size01\n",
    "            size01 = int(res.stderr.decode('utf-8').split()[-elem_cnt+4].replace('Size01=', ''))\n",
    "                       \n",
    "            # Size04\n",
    "            size04 = int(res.stderr.decode('utf-8').split()[-elem_cnt+5].replace('Size04=', ''))\n",
    "            \n",
    "            # Size09\n",
    "            size09 = int(res.stderr.decode('utf-8').split()[-elem_cnt+6].replace('Size09=', ''))\n",
    "\n",
    "            # Size29\n",
    "            size29 = int(res.stderr.decode('utf-8').split()[-elem_cnt+7].replace('Size29=', ''))\n",
    "            \n",
    "            # Size30\n",
    "            size30 = int(res.stderr.decode('utf-8').split()[-elem_cnt+8].replace('Size30=', ''))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Error: seed={}'.format(seed))\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        solve_result.append(score)\n",
    "        solve_result.append(rel_score)\n",
    "\n",
    "        solve_result.append(block_cnt)\n",
    "        solve_result.append(disuse_block_size)\n",
    "        \n",
    "        solve_result.append(size01)\n",
    "        solve_result.append(size04)\n",
    "        solve_result.append(size09)\n",
    "        solve_result.append(size29)\n",
    "        solve_result.append(size30)\n",
    "        solve_result.append(result)\n",
    "\n",
    "        result_df = pd.concat([result_df, pd.DataFrame(solve_result).T], axis=0)\n",
    "\n",
    "    logger.info('finish!')\n",
    "    \n",
    "    # 結果を整形\n",
    "    result_df.index = range(result_df.shape[0])\n",
    "    result_df.columns = ['testset', 'seed', 'D', 'time', 'score', 'rel_score', 'block_cnt', 'disuse_block_size', 'size01', 'size04', 'size09', 'size29', 'size30', 'result']\n",
    "\n",
    "    # 自己相対スコアを算出\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_df(result_df):\n",
    "    # 全体サマリ\n",
    "    summary_all_df = pd.DataFrame()\n",
    "\n",
    "    for testset in np.unique(result_df['testset']):\n",
    "        test_result_df = result_df.query('testset == \"{}\"'.format(testset))\n",
    "\n",
    "        summary_df = pd.DataFrame(\n",
    "        {\n",
    "            'testset': [testset],\n",
    "            \n",
    "            'time_mean': [int(np.mean(test_result_df['time']))],\n",
    "            \n",
    "            'score_mean': [int(np.mean(test_result_df['score']))],\n",
    "            'score_min': [min(test_result_df['score'])],\n",
    "            'score_max': [max(test_result_df['score'])],\n",
    "\n",
    "            'rel_score_mean': [int(np.mean(test_result_df['rel_score']))],\n",
    "            'rel_score_min': [min(test_result_df['rel_score'])],\n",
    "            'rel_score_max': [max(test_result_df['rel_score'])],\n",
    "\n",
    "            'block_cnt_mean': [np.mean(test_result_df['block_cnt'])],\n",
    "            'block_cnt_min': [min(test_result_df['block_cnt'])],\n",
    "            'block_cnt_max': [max(test_result_df['block_cnt'])],\n",
    "\n",
    "            'disuse_block_size_mean': [np.mean(test_result_df['disuse_block_size'])],\n",
    "            'disuse_block_size_min': [min(test_result_df['disuse_block_size'])],\n",
    "            'disuse_block_size_max': [max(test_result_df['disuse_block_size'])],\n",
    "\n",
    "            'size01_mean': [np.mean(test_result_df['size01'])],\n",
    "            'size04_mean': [np.mean(test_result_df['size04'])],\n",
    "            'size09_mean': [np.mean(test_result_df['size09'])],\n",
    "            'size29_mean': [np.mean(test_result_df['size29'])],\n",
    "            'size30_mean': [np.mean(test_result_df['size30'])],\n",
    "\n",
    "            'time_max': [max(test_result_df['time'])],\n",
    "        })\n",
    "\n",
    "        summary_all_df = pd.concat([summary_all_df, summary_df], axis=0)   \n",
    "\n",
    "    summary_all_df['tag'] = PROG_TAG\n",
    "    \n",
    "    cols = ['tag']\n",
    "    cols.extend(summary_df.columns)\n",
    "    \n",
    "    summary_all_df = summary_all_df[cols]\n",
    "    \n",
    "    return summary_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 15:46:03,187: Start\n",
      "2023-03-30 15:46:15,029: finish!\n"
     ]
    }
   ],
   "source": [
    "PROG_NAME_LIST = ['main_tune']\n",
    "\n",
    "testset_name = '01_testset_pre'\n",
    "#testset_name = '04_testset_param'\n",
    "#testset_name = '02_testset_sys'\n",
    "#testset_name = '03_testset_stress'\n",
    "\n",
    "result_dict = {}\n",
    "summary_all_dict = {}\n",
    "\n",
    "for PROG_NAME in PROG_NAME_LIST:\n",
    "    prog_path = os.path.join(PRJ_DIR, PROG_NAME)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    testset_result_df = run_test(testset_name, 68, 1)\n",
    "    result_df = pd.concat([result_df, testset_result_df], axis=0)\n",
    "    \n",
    "    result_dict[PROG_NAME] = result_df\n",
    "    summary_all_dict[PROG_NAME] = get_summary_df(result_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(testset_result_df['block_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(param_list):\n",
    "    invalid_shift, invalid_clear = param_list\n",
    "    \n",
    "    testset_name = '04_testset_param'\n",
    "    testset_result_df = run_test(testset_name, invalid_shift=invalid_shift, invalid_clear=invalid_clear)\n",
    "    \n",
    "    score = sum(testset_result_df['block_cnt'])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    invalid_shift = trial.suggest_int('invalid_shift', 30, 99) \n",
    "    invalid_clear = trial.suggest_int('invalid_clear', 1, 20) \n",
    "    \n",
    "    #valid_add = trial.suggest_int('valid_add', 1, 99) \n",
    "    #valid_shift = trial.suggest_int('valid_shift', 1, 99) \n",
    "    #valid_redundant = trial.suggest_int('valid_redundant', 1, 99) \n",
    "    #valid_clear = trial.suggest_int('valid_clear', 1, 20) \n",
    "\n",
    "    return calc_score([invalid_shift, invalid_clear]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-30 16:05:06,867]\u001b[0m A new study created in memory with name: no-name-104bcd88-f1a7-4cd6-a818-2336e91665e1\u001b[0m\n",
      "2023-03-30 16:05:06,868: Start\n",
      "2023-03-30 16:05:28,749: finish!\n",
      "\u001b[32m[I 2023-03-30 16:05:28,750]\u001b[0m Trial 0 finished with value: 1834.0 and parameters: {'invalid_shift': 68, 'invalid_clear': 1}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:05:28,751: Start\n",
      "2023-03-30 16:05:49,202: finish!\n",
      "\u001b[32m[I 2023-03-30 16:05:49,204]\u001b[0m Trial 1 finished with value: 2043.0 and parameters: {'invalid_shift': 97, 'invalid_clear': 13}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:05:49,205: Start\n",
      "2023-03-30 16:06:10,889: finish!\n",
      "\u001b[32m[I 2023-03-30 16:06:10,890]\u001b[0m Trial 2 finished with value: 1969.0 and parameters: {'invalid_shift': 65, 'invalid_clear': 12}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:06:10,891: Start\n",
      "2023-03-30 16:06:30,843: finish!\n",
      "\u001b[32m[I 2023-03-30 16:06:30,844]\u001b[0m Trial 3 finished with value: 2047.0 and parameters: {'invalid_shift': 94, 'invalid_clear': 3}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:06:30,845: Start\n",
      "2023-03-30 16:06:52,495: finish!\n",
      "\u001b[32m[I 2023-03-30 16:06:52,496]\u001b[0m Trial 4 finished with value: 2022.0 and parameters: {'invalid_shift': 63, 'invalid_clear': 19}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:06:52,497: Start\n",
      "2023-03-30 16:07:14,628: finish!\n",
      "\u001b[32m[I 2023-03-30 16:07:14,629]\u001b[0m Trial 5 finished with value: 1902.0 and parameters: {'invalid_shift': 67, 'invalid_clear': 6}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:07:14,630: Start\n",
      "2023-03-30 16:07:35,091: finish!\n",
      "\u001b[32m[I 2023-03-30 16:07:35,092]\u001b[0m Trial 6 finished with value: 1906.0 and parameters: {'invalid_shift': 85, 'invalid_clear': 3}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:07:35,093: Start\n",
      "2023-03-30 16:07:56,208: finish!\n",
      "\u001b[32m[I 2023-03-30 16:07:56,209]\u001b[0m Trial 7 finished with value: 1858.0 and parameters: {'invalid_shift': 77, 'invalid_clear': 3}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:07:56,210: Start\n",
      "2023-03-30 16:08:18,952: finish!\n",
      "\u001b[32m[I 2023-03-30 16:08:18,953]\u001b[0m Trial 8 finished with value: 1976.0 and parameters: {'invalid_shift': 62, 'invalid_clear': 13}. Best is trial 0 with value: 1834.0.\u001b[0m\n",
      "2023-03-30 16:08:18,954: Start\n",
      "2023-03-30 16:08:41,754: finish!\n",
      "\u001b[32m[I 2023-03-30 16:08:41,755]\u001b[0m Trial 9 finished with value: 1848.0 and parameters: {'invalid_shift': 54, 'invalid_clear': 3}. Best is trial 0 with value: 1834.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.enqueue_trial({'invalid_shift': 68, 'invalid_clear': 1})\n",
    "\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'invalid_shift': 68, 'invalid_clear': 1}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1944"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
